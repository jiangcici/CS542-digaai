{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch train loss at step 0 : 0.69743407\n",
      "Minibatch train error: 4.000%\n",
      "Test loss: 0.769\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 50 : 0.00032574456\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 16.090\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 100 : 0.00022406428\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 16.817\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 150 : 0.0001793289\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 17.261\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 200 : 0.00014717813\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 17.655\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 250 : 0.00012302442\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 18.013\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 300 : 0.00010440356\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 18.341\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 350 : 8.96874e-05\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 18.644\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 400 : 7.789685e-05\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 18.926\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 450 : 6.828895e-05\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 19.189\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 500 : 6.0375303e-05\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 19.434\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 550 : 5.37773e-05\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: 19.666\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 600 : 9.691645\n",
      "Minibatch train error: 4.000%\n",
      "Test loss: 18.605\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 650 : 0.07722251\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -2.473\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 700 : 0.032639194\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -3.386\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 750 : 0.020572592\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -3.861\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 800 : 0.014890044\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -4.191\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 850 : 0.011486466\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -4.454\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 900 : 0.009309887\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -4.667\n",
      "Test error: 4.000%\n",
      "Minibatch train loss at step 950 : 0.007811728\n",
      "Minibatch train error: 0.000%\n",
      "Test loss: -4.844\n",
      "Test error: 4.000%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle as p\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# set variables\n",
    "tweet_size = 25\n",
    "hidden_size = 100\n",
    "vocab_size = 41\n",
    "batch_size = 4\n",
    "\n",
    "def one_hot(raw_data, vocab_size):\n",
    "    data = np.zeros((len(raw_data), tweet_size, vocab_size))\n",
    "    for tweet_index in range(len(raw_data)):\n",
    "        tweet = raw_data[tweet_index]\n",
    "        for word_index in range(tweet_size):\n",
    "            word_id = tweet[word_index]\n",
    "            data[tweet_index, word_index, word_id] = 1\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create a session variable that we can run later. \n",
    "session = tf.Session()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# make the lstm cells, and wrap them in MultiRNNCell for multiple layers\n",
    "lstm_cell_1 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "lstm_cell_2 = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "multi_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1, lstm_cell_2] , state_is_tuple=True)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "_, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32)\n",
    "\n",
    "# a useful function that takes an input and what size we want the output \n",
    "# to be, and multiples the input by a weight matrix plus bias (also creating\n",
    "# these variables)\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable(\"weight_matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state \n",
    "# of the LSTM\n",
    "sentiment = linear(final_state[-1][-1], 1, name=\"output\")\n",
    "\n",
    "\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "\n",
    "# define cross entropy loss function\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# round our actual probabilities to compute error\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "pred_err = tf.to_float(tf.not_equal(prediction, labels))\n",
    "pred_err = tf.reduce_sum(pred_err)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# initialize any variables\n",
    "tf.global_variables_initializer().run(session=session)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('trainData.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('testData.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "# we are just taking the first 1000 things from the test set for faster evaluation\n",
    "test_data = test_data[0:25] \n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "one_hot_test_tweets = one_hot(test_tweets, vocab_size)\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "# we'll train with batches of size 128.  This means that we run \n",
    "# our model on 128 examples and then do gradient descent based on the loss\n",
    "# over those 128 examples.\n",
    "num_steps = 1000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # get data for a batch\n",
    "    offset = (step * batch_size) % (len(train_data) - batch_size)\n",
    "    batch_tweets = one_hot(train_tweets[offset : (offset + batch_size)], vocab_size)\n",
    "    batch_labels = train_labels[offset : (offset + batch_size)]\n",
    "    \n",
    "    # put this data into a dictionary that we feed in when we run \n",
    "    # the graph.  this data fills in the placeholders we made in the graph.\n",
    "    data = {tweets: batch_tweets, labels: batch_labels}\n",
    "    \n",
    "    # run the 'optimizer', 'loss', and 'pred_err' operations in the graph\n",
    "    _, loss_value_train, error_value_train = session.run(\n",
    "      [optimizer, loss, pred_err], feed_dict=data)\n",
    "    \n",
    "    # print stuff every 50 steps to see how we are doing\n",
    "    if (step % 50 == 0):\n",
    "        print(\"Minibatch train loss at step\", step, \":\", loss_value_train)\n",
    "        print(\"Minibatch train error: %.3f%%\" % error_value_train)\n",
    "        \n",
    "        # get test evaluation\n",
    "        test_loss = []\n",
    "        test_error = []\n",
    "        for batch_num in range(int(len(test_data)/batch_size)):\n",
    "            test_offset = (batch_num * batch_size) % (len(test_data) - batch_size)\n",
    "            test_batch_tweets = one_hot_test_tweets[test_offset : (test_offset + batch_size)]\n",
    "            test_batch_labels = test_labels[test_offset : (test_offset + batch_size)]\n",
    "            data_testing = {tweets: test_batch_tweets, labels: test_batch_labels}\n",
    "            loss_value_test, error_value_test = session.run([loss, pred_err], feed_dict=data_testing)\n",
    "            test_loss.append(loss_value_test)\n",
    "            test_error.append(error_value_test)\n",
    "        \n",
    "        print(\"Test loss: %.3f\" % np.mean(test_loss))\n",
    "        print(\"Test error: %.3f%%\" % np.mean(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
